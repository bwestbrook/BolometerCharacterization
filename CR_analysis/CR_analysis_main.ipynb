{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7859bb5-e5dc-4907-bfa2-82fa896ee238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosmic Ray Analysis Code by Shawn - main function outline:\n",
      " 1 Import data file\n",
      " 2 Modify data - rescale, detrend, remove re-zeros, etc.\n",
      " 3 Matched filter peak finder\n",
      " 4 Initial analysis tools - spectrum analyzer, etc.\n",
      " 5 Fitter and fit rejection\n",
      " 6 Coincidence analysis\n",
      " 7 Plot\n"
     ]
    }
   ],
   "source": [
    "print('Cosmic Ray Analysis Code by Shawn - main function outline:', end = '\\n 1 ')\n",
    "print('Import data file', end = '\\n 2 ')\n",
    "print('Modify data - rescale, detrend, remove re-zeros, etc.', end = '\\n 3 ')\n",
    "print('Matched filter peak finder', end = '\\n 4 ')\n",
    "print('Initial analysis tools - spectrum analyzer, etc.', end = '\\n 5 ')\n",
    "print('Fitter and fit rejection', end = '\\n 6 ')\n",
    "print('Coincidence analysis', end = '\\n 7 ')\n",
    "print('Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9757a751-626a-42f9-8cb4-41e195d55dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports\n"
     ]
    }
   ],
   "source": [
    "print('Imports')\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import pylab as pl\n",
    "#from DFT import *\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "from scipy import signal\n",
    "from scipy import optimize\n",
    "from scipy import math\n",
    "from scipy.fftpack import rfft, irfft, fftfreq, ifft, fft\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.integrate as integrate\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import shutil\n",
    "import statistics\n",
    "from statistics import median_low\n",
    "from matplotlib import cm\n",
    "from matplotlib import mlab\n",
    "from math import exp, expm1\n",
    "import sys\n",
    "import bisect\n",
    "import os\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef471e1b-a7bd-4c6d-bddb-d6e7410134d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining things\n"
     ]
    }
   ],
   "source": [
    "print('Defining things')\n",
    "V2uA_SQ1 = 25.2 #define SQUID calibrations in uA/V\n",
    "V2uA_SQ2 = 25.2\n",
    "V2uA_SQ3 = 30.0\n",
    "V2uA_SQ4 = 30.0\n",
    "V2uA_SQ5 = 25.9\n",
    "V2uA_SQ6 = 25.2\n",
    "\n",
    "xaxes = []\n",
    "ymeansubs = []\n",
    "timeconsts = []\n",
    "energyhists = []\n",
    "rates = []\n",
    "allphases = []\n",
    "maxvalsxs = []\n",
    "maxvalsys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4052a90-afdc-4461-ba48-209aa234d844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting functions\n"
     ]
    }
   ],
   "source": [
    "print('Supporting functions')\n",
    "\n",
    "def extract_raw_data(fn,colNames):\n",
    "    '''\n",
    "    Extract txt data from a txt file fn and return\n",
    "    a Python dictionary. colNames is a list of strings\n",
    "    corresponding to each column in the txt file.\n",
    "\n",
    "    d['filename'] returns the filename.\n",
    "    d[column name] returns an array with the data from that column.\n",
    "    '''\n",
    "    \n",
    "    f = open(fn)\n",
    "    d = {}\n",
    "    d['filename'] = fn\n",
    "    llist = []\n",
    "    for i in range(len(colNames)):\n",
    "        llist.append([])\n",
    "    for line in f:\n",
    "        lineSplit = line.split()\n",
    "        for i in range(len(colNames)):\n",
    "            llist[i].append(float(lineSplit[i].replace(',','')))\n",
    "    for i in range(len(colNames)):\n",
    "        d[colNames[i]] = pl.array(llist[i])\n",
    "    f.close()\n",
    "    return d\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "def extract_raw_ts(fn):\n",
    "    '''\n",
    "    Extract XY data from a txt file fn and\n",
    "    return a Python dictionary with entries x and y.\n",
    "\n",
    "    d['filename'] returns the filename.\n",
    "    '''\n",
    "    \n",
    "    colNames = ['y0','y1','y2','y3','y4','y5','y6','y7']\n",
    "    #colNames = ['y0','y1']\n",
    "    return extract_raw_data(fn,colNames)\n",
    "\n",
    "def extract_raw_ts_3ch(fn):\n",
    "    colNames = ['y0','y1', 'y2']\n",
    "    return extract_raw_data(fn,colNames)\n",
    "\n",
    "def extract_raw_ts_1ch(fn):\n",
    "    colNames = ['y0']\n",
    "    return extract_raw_data(fn,colNames)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# Template function for the matched filter peak finder\n",
    "\n",
    "def template(x):\n",
    "    xsc = 100.\n",
    "    relsc = 100./7.\n",
    "    phs = 0.4975\n",
    "    if(x<0.5): \n",
    "        return 1/(1+math.exp(-xsc*relsc*(x-phs)))\n",
    "    else:\n",
    "        return math.exp(-xsc*(x-0.499848))\n",
    "\n",
    "def filtertempl(x):\n",
    "    return 1 - math.exp(-(10.*x))\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# Function to combine two data sets (legacy)\n",
    "\n",
    "def CR_combine(fnList,newfn):\n",
    "    with open(newfn + '.dat','wb') as wfd:\n",
    "        for f in fnList:\n",
    "            with open(f,'rb') as fd:\n",
    "                shutil.copyfileobj(fd, wfd, 1024*1024*10)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# Functions to find the closest value to a specified value in an array\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    minarr = array - value\n",
    "    idx = np.argmin(i for i in minarr if i > 0)\n",
    "    idxx = (np.abs(array - value)).argmin()\n",
    "\n",
    "    return array[idxx].tolist()\n",
    "\n",
    "def find_nearest_fix(array, value):\n",
    "    array = np.asarray(array)\n",
    "    minarr = array - value\n",
    "    idx = np.argmin(i for i in minarr if i > 0)\n",
    "    idxx = (np.abs(array - value)).argmin()\n",
    "\n",
    "    return array[idx].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3354e75a-ef8a-49ab-99a2-b65f205d3b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of the primary function\n"
     ]
    }
   ],
   "source": [
    "print('Start of the primary function')\n",
    "\n",
    "\n",
    "def CR_analyze(data, sets, SQs, gain, bias, scantime, inv, pklsuffix, savefits = True, savefigs = True, showfits=False, \\\n",
    "               fluxfix = True, noisefilt = False, coincidence = False, verbose=True, detrend=True, pwrcnv = False, fit = True):\n",
    "\n",
    "\n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Parameter input explanations:\n",
    "    \n",
    "    data: The name of the data file, up to the number. Should always end in \"00x.txt\", so this input is \"dataname_\" and the \"00x.txt\" is added to the string based on the 'sets' input\n",
    "    sets: A two value array of format [beginning data set, end data set]\n",
    "    SQs: Right now this is set up for three column inputs, so the inupt is of format [1,2,3] if the data was being taken with SQs 1, 2, and 3 for example\n",
    "    gain: An array of the channel gains used in the power conversion\n",
    "    bias: An array of biases used\n",
    "    scantime: Length of each data set in seconds\n",
    "    inv: An array to invert each channel or not\n",
    "    plksuffix: Suffix used to name the pickle files, plot titles, etc. Should describe exactly what you're running\n",
    "    savefits: Saves pngs of each fit to a new directory\n",
    "    savefigs: Saves energy and time constant histograms, as well as text files of relevant run info\n",
    "    showfits: Shows each pulse fit in real time\n",
    "    fluxfix: Overwrites timestreams with fixed jumped flux events\n",
    "    noisefilt: A work in progress function to remove certain noise components using a notch fitler applied to the power spectra\n",
    "    coincidence: Looks at coincidence events and prints out a pickle file with relevant info\n",
    "    verbose: Outputs more information about jumped flux removal and eventually fits\n",
    "    detrend: Detrends each timestream\n",
    "    inv: Inverts the data. Right now we'll need to know before running if the peaks need to be inverted for this input. Will change this to detect automatically in the future.\n",
    "    pwrcnv: Converts y to units of power\n",
    "    fit: Option to fit the data\n",
    "    \n",
    " \n",
    "\n",
    "    #Example parameter inputs: \n",
    "        # CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_05_16-control_nosource/CR_Scan_002/_\"], \\\n",
    "              # [2,259], [1,2,6], [100.,100.,100.], [3.457,3.187,3.165], 300., [False, False, True], [\"20210516_controlbolos_nosource_2-256\"], \\\n",
    "              # showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "    \n",
    "    # note: will add input for number of channels in the future\n",
    "   \n",
    "   '''\n",
    "    \n",
    "\n",
    "   \n",
    "    pl.clf()\n",
    "    datalist = []\n",
    "    jfe_datasets = []\n",
    "    energies = [[],[],[]]\n",
    "    energies_data = [[],[],[]]\n",
    "    energies_nojfes = [[],[],[]]\n",
    "    timeconsts = [[],[],[]]\n",
    "    rates = [[],[],[]]\n",
    "    peakvals = [[],[],[]]\n",
    "    errors = []\n",
    "    e_tot = [] #[[datalists][channels][energies]]\n",
    "    tau_tot = [] #[[datalists][channels][tau]]\n",
    "    rt_tot = [] #[[datalists][channels][risetimes]]\n",
    "    coinc_tot = [] #[[datalists][x value]\n",
    "    \n",
    "    \n",
    "    # Defining the string to find the correct data set based on function inputs\n",
    "    \n",
    "    if len(sets)==1:\n",
    "        if sets[0] < 10:\n",
    "            dl = data[0] + \"00\" + str(sets[0]) + \".txt\"\n",
    "        if sets[0] >= 10 and sets[0] < 100:\n",
    "            dl = data[0] + \"0\" + str(sets[0]) + \".txt\"\n",
    "        if sets[0] >= 100:\n",
    "            dl = data[0] + str(sets[0]) + \".txt\"\n",
    "        datalist.append(dl)\n",
    "    else:\n",
    "        setnumb = len(range(sets[0], sets[1]+1))\n",
    "        sn=[]\n",
    "        for i in range(setnumb):\n",
    "            sn.append(sets[0]+i) # Make an array that starts with the first dataset number you input\n",
    "            if sn[i] < 10:\n",
    "                dl = data[0] + \"00\" + str(sn[i]) + \".txt\"\n",
    "            if sn[i] >= 10 and i < 100:\n",
    "                dl = data[0] + \"0\" + str(sn[i]) + \".txt\"\n",
    "            if sn[i] >= 100:\n",
    "                dl = data[0] + str(sn[i]) + \".txt\"\n",
    "            datalist.append(dl)\n",
    "            e_tot.append(sn[i]) # Count up from first dataset\n",
    "            tau_tot.append(sn[i])\n",
    "            rt_tot.append(sn[i])\n",
    "            coinc_tot.append(sn[i])\n",
    "            \n",
    "    # Defining the directory\n",
    "    numslashes = 9\n",
    "    slashes = []\n",
    "    for i in range(len(data[0])):\n",
    "        if data[0][i] =='/':\n",
    "            slashes.append(0)\n",
    "        if len(slashes) == numslashes:\n",
    "            writepath = data[0][:i+1]\n",
    "            break\n",
    "    wpsuffix = pklsuffix[0]\n",
    "            \n",
    "     \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "    \n",
    "    # Start of primary for loop\n",
    "    \n",
    "    for n in range(len(datalist)): \n",
    "        if verbose:\n",
    "            print(datalist[n])   \n",
    "        print(\"running data set\", datalist[n][-7:-4])\n",
    "        dList0 = []\n",
    "        jfe_channels = [] #jumped flux events\n",
    "        \n",
    "        \n",
    "        d = extract_raw_ts_3ch(datalist[n])\n",
    "        dList0.append(d)\n",
    "        ch = []\n",
    "        ymeansub = [] # main y data\n",
    "        \n",
    "        notchyms = []\n",
    "        maxvalsy = []\n",
    "        maxvalsx = []\n",
    "    \n",
    "        # Telling the code which SQUIDs you're using\n",
    "        for j in range(len(SQs)): \n",
    "            ymeansub_disc = []\n",
    "            c = \"y\"+ str(j)\n",
    "            ch.append(c)\n",
    "            print(\"channel\", SQs[j])\n",
    "        \n",
    "            #Assign SQ calibrations\n",
    "            \n",
    "            if SQs[j]==1:\n",
    "                V2uA0 = V2uA_SQ1\n",
    "            else:\n",
    "                if SQs[j]==2:\n",
    "                    V2uA0 = V2uA_SQ2\n",
    "                else:\n",
    "                    if SQs[j]==3:\n",
    "                        V2uA0 = V2uA_SQ3\n",
    "                    else:\n",
    "                        if SQs[j]==4:\n",
    "                            V2uA0 = V2uA_SQ4\n",
    "                        else:\n",
    "                            if SQs[j]==5:\n",
    "                                V2uA0 = V2uA_SQ5\n",
    "                            else:\n",
    "                                if SQs[j]==6:\n",
    "                                    V2uA0 = V2uA_SQ6\n",
    "            \n",
    "           \n",
    "            # Defining the y-scale\n",
    "            if pwrcnv:\n",
    "                yList0 = (10**6)*(10**-5)*bias[j]*V2uA0*dList0[0][ch[j]]/gain[j] \n",
    "            else:\n",
    "                yList0 = dList0[0][ch[j]]    \n",
    "\n",
    "            # Define samplerate in Hz\n",
    "            samplerate = len(yList0)/scantime\n",
    "            \n",
    "            # 'Normalize' x\n",
    "            xList0 = range(0,len(yList0))\n",
    "            xNorm0 = (scantime/len(xList0))\n",
    "            xLn0 = []\n",
    "            for i in range(len(xList0)):\n",
    "                normd = xNorm0*xList0[i]\n",
    "                xLn0.append(normd)\n",
    "\n",
    "                \n",
    "                \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "            \n",
    "            \n",
    "    # Fixing jumped flux\n",
    "            \n",
    "            # Finding discontinuities \n",
    "            deryList0 = np.gradient(yList0) # Take the first derivative of the y-data\n",
    "            discthresh = 50*np.std(deryList0) # Set the threshold for peakfinding, 50 standard deviations works well\n",
    "            \n",
    "            maxsx = []\n",
    "            maxsy = []\n",
    "            xdisc_N = []\n",
    "            \n",
    "            # This is the condition for finding the JFEs - if the data point is above the threshold value AND the \n",
    "            # difference in the mean of the 500 points before and after the event is also above the threshold\n",
    "            \n",
    "            for i in range(len(yList0)):\n",
    "                if abs(deryList0[i]) > discthresh and abs(np.mean(yList0[i - int(samplerate*0.1):i])-np.mean(yList0[i:i + int(samplerate*0.1)])) > discthresh:\n",
    "                    maxsx.append(xLn0[i])\n",
    "                    maxsy.append(deryList0[i]) # Make a list of values above the threshold\n",
    "            \n",
    "            if len(maxsy) > 0: # If discontinuities are found \n",
    "                \n",
    "                # Finding the x location of the discontinuity\n",
    "                maxtime = maxsx[maxsy.index(max(maxsy))] # Find the x location of the max value\n",
    "                roundedx = np.round(maxsx, 2) # Round the x values to find general locations\n",
    "                xdisc = np.unique(roundedx) # Insert rounded x values into their own array, N_array = N_disc.\n",
    "                roundxmain = np.array(np.round(xLn0, 2)) # Round all x data\n",
    "                \n",
    "                \n",
    "                # Removing discontinuities\n",
    "                remove = int(samplerate * 0.015) # Number of points to remove before disc location\n",
    "                remove_last = int(samplerate * 0.01) # Number of point to remove after disc location. Charactaristic of most jumped flux recover times - would like to make mroe robust in the future\n",
    "                ind1 = []\n",
    "                ind2 = []\n",
    "                \n",
    "                for i in range(len(xdisc)): # Loop over each found discontinuity\n",
    "                    xdisc_N.append(np.where(roundxmain == xdisc[i])[0][0]) # Find index of x locations\n",
    "                    \n",
    "                    if len(xdisc) == 1: # If there's only one discontinuity detected\n",
    "                        ymeansub_disc.extend(yList0[:xdisc_N[i]] - np.mean(yList0[:xdisc_N[i]])) # Remove DC offset before disc.\n",
    "                        ymeansub_disc.extend(yList0[xdisc_N[i]:] - np.mean(yList0[xdisc_N[i]:])) # Remove DC offset after disc.\n",
    "                    else:\n",
    "                        if i == 0: # Remove DC offset before first disc.\n",
    "                            ymeansub_disc.extend(yList0[:xdisc_N[i]] - np.mean(yList0[:xdisc_N[i]]))\n",
    "                        if i != len(xdisc)-1 and i != 0: # Remove DC offset after last disc., before next\n",
    "                            ymeansub_disc.extend(yList0[xdisc_N[i-1]:xdisc_N[i]] - np.mean(yList0[xdisc_N[i-1]:xdisc_N[i]]))\n",
    "                        if i == len(xdisc)-1: # Remove DC offset after the last disc.\n",
    "                            ymeansub_disc.extend(yList0[xdisc_N[i-1]:xdisc_N[i]] - np.mean(yList0[xdisc_N[i-1]:xdisc_N[i]]))\n",
    "                            ymeansub_disc.extend(yList0[xdisc_N[i]:] - np.mean(yList0[xdisc_N[i]:]))\n",
    "                    \n",
    "                    # Replace datapoints where disc. occurs with mean values\n",
    "                    ind1.append(xdisc_N[i]-remove)\n",
    "                    ind2.append(xdisc_N[i]+remove_last)\n",
    "                \n",
    "                for i in range(len(ind1)):\n",
    "                    ymeansub_disc[ind1[i]:ind2[i]] = [np.mean(ymeansub_disc)]*len(ymeansub_disc[ind1[i]:ind2[i]])\n",
    "                if fluxfix:\n",
    "                    yList0 = ymeansub_disc # Overwrite y data\n",
    "                \n",
    "            \n",
    "            # Some print options for jumped flux info\n",
    "            # These lines also append the JFE ararys\n",
    "            \n",
    "            if verbose: \n",
    "                if len(maxsy) == 0:\n",
    "                    print(\"No jumped flux events found in channel %i, dataset \"%j + datalist[n][-7:-3])\n",
    "                    jfe_channels.extend([0])\n",
    "                if len(maxsy) > 0:\n",
    "                    if len(xdisc) == 1:\n",
    "                        print(\"One jumped flux event found in channel %i, dataset \"%j + datalist[n][-7:-3])\n",
    "                        jfe_channels.extend([1])\n",
    "                if len(maxsy) > 0:\n",
    "                    if len(xdisc) > 1:\n",
    "                        print(\"%i \"%len(xdisc) + \"jumped flux events found in channel %i, dataset \"%j + datalist[n][-7:-3])\n",
    "                        jfe_channels.extend([len(xdisc)])\n",
    "            else:\n",
    "                if len(maxsy) == 0:\n",
    "                    jfe_channels.extend([0])\n",
    "                if len(maxsy) > 0:\n",
    "                    if len(xdisc) >= 1:\n",
    "                        jfe_channels.extend([len(xdisc)])\n",
    "                        \n",
    "                      \n",
    "            \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "            \n",
    "    # Remaining dataset processing\n",
    "        \n",
    "            # Detrend data\n",
    "            \n",
    "            if detrend:\n",
    "                yList0det = signal.detrend(yList0) #detrend raw data\n",
    "            else:\n",
    "                yList0det = yList0\n",
    "            \n",
    "            # Invert\n",
    "            if inv[j]:\n",
    "                for i in range(len(yList0det)):\n",
    "                    yList0det[i] = -1.*yList0det[i]\n",
    "\n",
    "            # First DC offset removal before finding noise threshold\n",
    "           \n",
    "            if len(maxsy) == 0:\n",
    "                ymeansub.append(yList0det - np.mean(yList0det))\n",
    "            else:\n",
    "                ymeansub.append(yList0det)\n",
    "            \n",
    "                      \n",
    "      \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "    \n",
    "    # Noise filtering code - not complete\n",
    "    # This code defines a notch filter to remove certain parts of the noise spectra\n",
    "    # Additions need to be made to output noise value from welch\n",
    "            \n",
    "            if noisefilt:\n",
    "                fft_freq, fft_psd = signal.welch(ymeansub[j], samplerate)\n",
    "                b, a = signal.iirnotch(215, 30, samplerate)\n",
    "                notchymssub = signal.lfilter(b, a, ymeansub[j], axis=- 1, zi=None)\n",
    "\n",
    "                \n",
    "                hpfilt100 = signal.butter(1, 100, 'hp', output = 'sos', fs=5000)\n",
    "                filtered100 = signal.sosfilt(hpfilt100, ymeansub[j])\n",
    "                \n",
    "                hpfilt20 = signal.butter(1, 20, 'hp', output = 'sos', fs=5000)\n",
    "                filtered20 = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                filtered20_0_float = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                filtered20_1_HS = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                filtered20_0_float_source = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                filtered20_1_HS_source = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "        \n",
    "               \n",
    "                if n==0 and j == 0:\n",
    "                    filtered20_0_ns = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                \n",
    "                if n==0 and j == 1:\n",
    "                    filtered20_1_ns = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                    \n",
    "                if n==0 and j == 2:\n",
    "                    filtered20_2_ns = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                    \n",
    "                if n==1 and j == 0:\n",
    "                    filtered20_0_s = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                \n",
    "                if n==1 and j == 1:\n",
    "                    filtered20_1_s = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                    \n",
    "                if n==1 and j == 2:\n",
    "                    filtered20_2_s = signal.sosfilt(hpfilt20, ymeansub[j])\n",
    "                \n",
    "                notchyms.append(notchymssub)\n",
    "                fft_freq2, fft_psd2 = signal.welch(notchyms[j], samplerate)\n",
    "                \n",
    "            \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        \n",
    "    # Peak finder\n",
    "            \n",
    "            if fit:\n",
    "                # Setup matched filter peak finder\n",
    "                xmf = np.arange(0.4, 0.6, 0.0001) #define window/xaxis for convolving function\n",
    "                ymf = []\n",
    "                for k in range(len(xmf)):\n",
    "                    ymf.append(template(xmf[k])) # Create matched filter convolving function\n",
    "\n",
    "                # Convolve templates with data, take derivative to find rise location, find max\n",
    "                pf = signal.fftconvolve(ymeansub[j], ymf, mode='same')\n",
    "                fftpf = rfft(pf)\n",
    "                yderiv = np.gradient(pf)\n",
    "                yderivdetr = signal.detrend(yderiv)\n",
    "                peakthresh = 8.0\n",
    "                ydstd = peakthresh*np.std(yderivdetr) # Seems to be a good threshold based on the noise of the data set and goodness of fits\n",
    "                ydpks = []\n",
    "                xydpks = []\n",
    "                yderivdetr_temp = []\n",
    "                ymeansub_temp = []\n",
    "                indextemp = []\n",
    "               \n",
    "                # Removing peaks from timestreams to get an accurate std of the noise\n",
    "                for i in range(len(yderiv)):\n",
    "                    \n",
    "                    if i>25 and 0.75*yderivdetr[i-25] > ydstd or yderivdetr[i-25] < -0.75*ydstd: #numbers tweaked to optimize peak removal\n",
    "                        yderivdetr_temp.append(0)\n",
    "                        ymeansub_temp.append(0) # Defining the 'temporary' y array to grab the standard deviation from\n",
    "                        indextemp.append(i)\n",
    "                        \n",
    "                    else:\n",
    "                        yderivdetr_temp.append(yderivdetr[i])\n",
    "                        ymeansub_temp.append(ymeansub[j][i])\n",
    "                \n",
    "                stddata = 3*np.std(ymeansub_temp) #3 times the standard deviation\n",
    "                ydstdtemp = []\n",
    "                stdtemp = []\n",
    "                \n",
    "                # Finding peaks using new std      \n",
    "                for i in range(len(yderiv)):\n",
    "                    ydstdtemp.append(ydstd)\n",
    "                    stdtemp.append(stddata)\n",
    "                    if yderivdetr[i] > stddata:\n",
    "                        ydpks.append(yderivdetr[i])\n",
    "                        xydpks.append(xLn0[i])\n",
    "                        \n",
    "                        \n",
    "                # Final DC offset removal using peak-removed data\n",
    "                ymeansub[j] = ymeansub[j] - np.mean(ymeansub_temp)\n",
    "                \n",
    "               \n",
    "                # Pick out max\n",
    "                mvalsx_init = []\n",
    "                mvalsy_init = []\n",
    "                mvalsx = []\n",
    "                mvalsy = []\n",
    "                for i in range(len(ydpks)):\n",
    "                    if (i+1) >= len(ydpks):\n",
    "                        break\n",
    "                    \n",
    "                    if (ydpks[i+1]<ydpks[i]) and (ydpks[i-1]<ydpks[i]):\n",
    "                        mvalsy_init.append(ydpks[i])\n",
    "                        mvalsx_init.append(xydpks[i])\n",
    "               \n",
    "                \n",
    "                # remove duplicate peaks that occur within 5 ms of each other\n",
    "                for i in range(len(mvalsx_init)):\n",
    "                    if (i+1) >= len(mvalsx_init):\n",
    "                        mvalsx.append(mvalsx_init[i])\n",
    "                        mvalsy.append(mvalsy_init[i])\n",
    "                        break\n",
    "                    if mvalsx_init[i+1] - mvalsx_init[i] > 0.005:\n",
    "                        mvalsx.append(mvalsx_init[i])\n",
    "                        mvalsy.append(mvalsy_init[i])\n",
    "                \n",
    "                \n",
    "                maxvalsx.append(mvalsx)\n",
    "                maxvalsy.append(mvalsy)\n",
    "                \n",
    "                \n",
    "                if len(maxvalsx[j]) > 0:\n",
    "                    rates[j].append(len(maxvalsx[j]))\n",
    "                else:\n",
    "                    rates[j].append(0)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "    \n",
    "    # Start fitting \n",
    "    # Fits pulses in three sections: Exponential tail, rising edge, and peak, the combines the three fits\n",
    "    \n",
    "                \n",
    "                if len(maxvalsx[j]) > 0:\n",
    "                    \n",
    "                    fitlistsx = []\n",
    "                    fitlistsx_sc = []\n",
    "                    fitlistsy = []\n",
    "                    datarange = int(0.02*samplerate) # Used to define sample window\n",
    "                    fitlistsx_re = []\n",
    "                    fitlistsx_sc_re = []\n",
    "                    fitlistsy_re = []\n",
    "                    fitlistsx_p = []\n",
    "                    fitlistsy_p = []\n",
    "                    fitlistsx_sc_p = []\n",
    "                    datarange_re = int(0.1*samplerate) \n",
    "                    datarange_p = int(0.005*samplerate)\n",
    "                    \n",
    "                    def glitchfit(x, a, b): #defining a second degree polynomial to fit the event data in log space\n",
    "                        return a + x**b\n",
    "                    \n",
    "                    \n",
    "                    for i in range(len(maxvalsx[j])):\n",
    "                        \n",
    "                        fitlistx = []\n",
    "                        fitlisty = []\n",
    "                        fitlistx_sc = []\n",
    "                        flaggedfits = []\n",
    "                        threesigmas = []\n",
    "                        fntointmods = []\n",
    "                        phases = []\n",
    "                        maxs = []\n",
    "                        polyorder = 12\n",
    "      \n",
    "                    # Defining pulse windows\n",
    "        \n",
    "                        for l in range(datarange):\n",
    "                            fitlistx.append(maxvalsx[j][i]+(float(l)/samplerate)) # For each event, add an x data point until the end of the defined window\n",
    "                            fitlistx_sc.append(int(samplerate*maxvalsx[j][i])+(l)) # Do the same but in units of samples in order to define y-index\n",
    "                            fitlisty = ymeansub[j][fitlistx_sc[0]:fitlistx_sc[-1]+1] # Define y window\n",
    "                            \n",
    "                        fitlistsx.append(fitlistx)\n",
    "                        fitlistsx_sc.append(fitlistx_sc)\n",
    "                        fitlistsy.append(fitlisty)\n",
    "\n",
    "\n",
    "                    # Start of fitter for exponential tail\n",
    "                    # Here we will define the pulse window, then move to log space, fit a first degree polynomial, then convert back\n",
    "                    \n",
    "                        phase = fitlistsx[i][np.argmax(fitlistsy[i])] # Defines the max of the y fit data's x location\n",
    "                        \n",
    "                        fitlistx_chop = fitlistsx[i][np.argmax(fitlistsy[i]):] # Selecting only the tails for fitting in log space\n",
    "                        fitlisty_chop = fitlistsy[i][np.argmax(fitlistsy[i]):]\n",
    "                        \n",
    "                        np.seterr(invalid='ignore') # Ignore nan error from negative values in log\n",
    "                        fitlistx_logchop = np.log(fitlistx_chop)  # Take the log of each event for fitting\n",
    "                        fitlisty_logchop = np.log(fitlisty_chop)\n",
    "                        \n",
    "                        # Remove nan values from the data and interpolate between them for fits\n",
    "                        nan_indices = np.isnan(fitlisty_logchop) # Logic array where nan = True\n",
    "                        real_indices = np.logical_not(nan_indices) # Inverse logic array\n",
    "                        real_data = fitlisty_logchop[real_indices] # Making array for interpolate function with nans removed\n",
    "                        if len(real_data) == 0:\n",
    "                            print(\"error in fit, moving to next\")\n",
    "                            errors.append(1)\n",
    "                            break\n",
    "                        \n",
    "                        interp_y = np.interp(nan_indices.nonzero()[0], real_indices.nonzero()[0], real_data) # Interpolate function\n",
    "                        fitlisty_logchop[nan_indices] = interp_y # Setting the indexed values of the zero points to the interpolated function's values\n",
    "            \n",
    "                        \n",
    "                        # This is a seed fit to provide seed values for a higher order fit\n",
    "                        coefs = poly.polyfit(fitlistx_logchop, fitlisty_logchop, 2) # Define a polynomial to fit the events in log space\n",
    "                        ffit = poly.polyval(fitlistx_logchop, coefs) # Creating the fit in log space\n",
    "                        xfit = np.exp(fitlistx_logchop) # Convert back from log\n",
    "                        ffitexp = np.exp(ffit)\n",
    "                        \n",
    "                        # To 0th order correct fit overshoot at start of exp. curve\n",
    "                        ffitexpmod = []\n",
    "                        for k in range(len(ffitexp)):\n",
    "                            if ffitexp[k] > np.max(fitlistsy[i]):\n",
    "                                ffitexpmod.append(np.max(fitlistsy[i]))\n",
    "                            else:\n",
    "                                ffitexpmod.append(ffitexp[k])\n",
    "                                \n",
    "            \n",
    "    \n",
    "        # Start fit for rising edge\n",
    "        \n",
    "                    # Define pulse windows, '_re' = 'rising edge'\n",
    "                    \n",
    "                        fitlistx_re = []\n",
    "                        fitlisty_re = []\n",
    "                        fitlistx_sc_re = []\n",
    "                        xshift = 10\n",
    "                        for l in range(datarange_re):\n",
    "                            fitlistx_re.append(maxvalsx[j][i] + (float(l)/samplerate) - (xshift/samplerate)) # For each event, subtract an x data point until the end of the defined window. Adding more points at the beginning to catch rising edge\n",
    "                            fitlistx_sc_re.append(int(samplerate*maxvalsx[j][i])+(l) - xshift) # Do the same but in units of samples in order to define y-index\n",
    "                            fitlisty_re = ymeansub[j][fitlistx_sc_re[0]:fitlistx_sc_re[-1]+1] # Define y window\n",
    "\n",
    "                        fitlistsx_re.append(fitlistx_re)\n",
    "                        fitlistsx_sc_re.append(fitlistx_sc_re)\n",
    "                        fitlistsy_re.append(fitlisty_re)\n",
    "\n",
    "                        \n",
    "                        # Start of fitter for rising edge\n",
    "                        # Here we'll define the rising edge's window and fit a line to it without going into log space\n",
    "                        \n",
    "                        if len(fitlistsy_re[i]) == 0: # Accounts for error in very few data sets where fitlistys_re[i] is not populated\n",
    "                            print(\"defined window invalid (line 676)\")\n",
    "                            errors.append(1)\n",
    "                            continue\n",
    "                        \n",
    "                        phase = fitlistsx_re[i][np.argmax(fitlistsy_re[i])] # Defines the max of the y fit data's x location\n",
    "                        \n",
    "                        fitlistx_re_chop = fitlistsx_re[i][:np.argmax(fitlistsy_re[i])] # Selecting only the rising edges\n",
    "                        fitlisty_re_chop = fitlistsy_re[i][:np.argmax(fitlistsy_re[i])]\n",
    "                        \n",
    "                        startrise = []\n",
    "                        meany = []\n",
    "                        fitlisty_re_chop_temp = []\n",
    "                        for l in range(len(fitlistx_re_chop)):\n",
    "                            if (l+1) >= len(fitlistx_re_chop):\n",
    "                                break\n",
    "                            \n",
    "                            fitlisty_re_chop_temp.append(fitlisty_re_chop[l])\n",
    "                            meany = 0.\n",
    "                            meanystd = 0.\n",
    "                            meany = np.mean(fitlisty_re_chop_temp)\n",
    "                            meanystd = np.std(fitlisty_re_chop_temp)\n",
    "                           \n",
    "                            if l > 5 and fitlisty_re_chop[l] - meany > 2*meanystd: # Get data for a few points first, then look to see if the difference from the current point to the mean is larger than 2x the standard deviation of the points \n",
    "                                rise_index = l - int(samplerate/5000)\n",
    "                                break\n",
    "                            else:\n",
    "                                rise_index = 0\n",
    "                        \n",
    "                        if rise_index == 0: #if it can't find the rising edge, lower the threshold\n",
    "                            for l in range(len(fitlistx_re_chop)):\n",
    "                                if (l+1) >= len(fitlistx_re_chop):\n",
    "                                    break\n",
    "                                if l > 5 and fitlisty_re_chop[l] - meany > 1.5*meanystd: # Get data for a few points first, then look to see if the difference from the current point to the mean is larger than 2x the standard deviation of the points \n",
    "                                    rise_index = l - int(samplerate/5000)\n",
    "                                    break\n",
    "                        \n",
    "                        fitlistx_re_fit = fitlistx_re_chop[rise_index:]\n",
    "                        fitlisty_re_fit = fitlisty_re_chop[rise_index:]\n",
    "                        \n",
    "                        index_last = rise_index + len(ffitexp)\n",
    "                        totalfitx = fitlistx_re[rise_index:index_last]\n",
    "                        totalfity = fitlisty_re[rise_index:index_last]\n",
    "                        \n",
    "                        coefs_re = poly.polyfit(fitlistx_re_fit, fitlisty_re_fit, 3) #polyorder) # Define a second degree polynomial to fit the events\n",
    "                        ffit_re = poly.polyval(fitlistx_re_fit, coefs_re) # Creating the fit\n",
    "                        \n",
    "                    \n",
    " \n",
    "                        \n",
    "            # Start fit for the peak\n",
    "        \n",
    "                    # Define pulse windows, 'p' = 'peak'\n",
    "                    \n",
    "                    \n",
    "                        fitlistx_p = []\n",
    "                        fitlisty_p = []\n",
    "                        fitlistx_sc_p = []\n",
    "                        xshift_p = -5\n",
    "                        for l in range(datarange_p):\n",
    "                            fitlistx_p.append(maxvalsx[j][i] + (float(l)/samplerate) - (xshift_p/samplerate)) # For each event, subtract an x data point until the end of the defined window. Adding more points at the beginning to catch rising edge\n",
    "                            fitlistx_sc_p.append(int(samplerate*maxvalsx[j][i])+(l) - xshift_p) # Do the same but in units of samples in order to define y-index\n",
    "                            fitlisty_p = ymeansub[j][fitlistx_sc_p[0]:fitlistx_sc_p[-1]+1] # Define y window\n",
    "\n",
    "                        fitlistsx_p.append(fitlistx_p)\n",
    "                        fitlistsx_sc_p.append(fitlistx_sc_p)\n",
    "                        fitlistsy_p.append(fitlisty_p)\n",
    "\n",
    "                        coefs_p = poly.polyfit(fitlistx_p, fitlisty_p, 10) # Define a second degree polynomial to fit the events\n",
    "              \n",
    "\n",
    "                        ffit_p = poly.polyval(fitlistx_p, coefs_p) # Creating the fit\n",
    "                        \n",
    "                        ########################################################\n",
    "   \n",
    "            # Combine the three fit functions\n",
    "\n",
    "                        # Setting up parameters and windows for combine funcitons\n",
    "        \n",
    "                        index_last = rise_index + len(ffitexp) + 100\n",
    "                        totalfitx = fitlistx_re[rise_index:index_last]\n",
    "                        totalfity = fitlisty_re[rise_index:index_last]\n",
    "                        phase_index = np.argmax(totalfity)\n",
    "                        totalfitx_re = totalfitx[:phase_index]\n",
    "                        totalfitx_exp = totalfitx[phase_index:]\n",
    "                        \n",
    "                        alpha1 = 100*len(totalfitx)\n",
    "                        alpha2 = 5*len(totalfitx)\n",
    "                        alpha3 = 10*len(totalfitx)\n",
    "                        phase1 = fitlistsx_re[i][np.argmax(fitlistsy_re[i])-10]\n",
    "                        phase2 = fitlistsx_re[i][np.argmax(fitlistsy_re[i])+0]\n",
    "                        phase3 = fitlistsx_re[i][np.argmax(fitlistsy_re[i])-10]\n",
    "                        function1 = []\n",
    "                        function2 = []\n",
    "                        function3 = []\n",
    "                        \n",
    "                        \n",
    "                        ffitexp_total = np.exp(poly.polyval(np.log(totalfitx), coefs))\n",
    "                        ffit_p_total = poly.polyval(totalfitx, coefs_p)\n",
    "                        ffitexp_totalmod = []\n",
    "                        ffit_re_total = poly.polyval(totalfitx, coefs_re)\n",
    "                        \n",
    "                        # This loop is to fix the issue of the start of the exponential fits going way above the peak in the data\n",
    "                        for k in range(len(ffitexp_total)):\n",
    "                            if ffitexp_total[k] > np.max(fitlistsy[i]):\n",
    "                                ffitexp_totalmod.append(np.max(fitlistsy[i]))\n",
    "                            else:\n",
    "                                ffitexp_totalmod.append(ffitexp_total[k])\n",
    " \n",
    "                        ffit_re_total = poly.polyval(totalfitx, coefs_re)\n",
    "                        ffitexp_total[:phase_index+3] = np.max(fitlistsy[i])\n",
    "                        ffit_re_total[phase_index-3:] = np.max(fitlistsy[i])\n",
    "                        \n",
    "                        # Creating the functions to define the combined function (funciton3)\n",
    "                        xx = np.linspace(totalfitx[0], totalfitx[-1], len(totalfitx))\n",
    "                        sigma1 = 1/(1+np.exp((-xx+phase1)*alpha1))\n",
    "                        sigma2 = 1/(1+np.exp((-xx+phase2)*alpha2))\n",
    "                        sigma3 = 1/(1+np.exp((-xx+phase3)*alpha3))\n",
    "                        for l in range(len(totalfitx)): \n",
    "                            function1.append((1 - sigma1[l]) * ffit_re_total[l]+sigma1[l] * ffit_p_total[l])\n",
    "                            function2.append((1 - sigma2[l]) * ffit_p_total[l]+sigma2[l] * ffitexp_totalmod[l])\n",
    "                            function3.append((1 - sigma3[l]) * ffit_re_total[l] +sigma3[l] * function2[l])\n",
    "                        \n",
    "                        #Record all values for histogramming\n",
    "                        energies[j].append(sum(function3))\n",
    "                        energies_data[j].append(sum(totalfity)/len(totalfity))\n",
    "                        peakvals[j].append(fitlistsx[i][np.argmax(fitlistsy[i])])\n",
    "                        \n",
    "\n",
    "                        #define params for time constants\n",
    "                        maxval = np.max(function3)\n",
    "                        maxvalindex = function3.index(maxval)\n",
    "                        onesigma = maxval*(1/np.exp(1))\n",
    "                        sigmaminx = totalfitx[ffitexp_totalmod.index(find_nearest(ffitexp_totalmod,onesigma))]\n",
    "                        sigmaminy = function3[ffitexp_totalmod.index(find_nearest(ffitexp_totalmod,onesigma))]\n",
    "                        thistau = 1*(sigmaminx - totalfitx[maxvalindex])\n",
    "   \n",
    "                        \n",
    "                        # Normalizing discontinuities from jumped flux\n",
    "                        gl_zeros = []\n",
    "                        normxdisc = []\n",
    "                        if len(xdisc_N) > 0:\n",
    "                            for k in range(len(xdisc_N)):\n",
    "                                gl_zeros.append(discthresh)\n",
    "                                normxdisc.append(xNorm0*xdisc_N[k])\n",
    "                          \n",
    "                        \n",
    "                        # Handler for if the fit finds a negative time constant, usually caused by bad higher order exp fits to the tail\n",
    "                        # Multiple degree polynomial fits for the tails usual fit them better, but sometimes can cause problems\n",
    "                        \n",
    "                        if thistau < 0.0:\n",
    "                       \n",
    "                            coefs = poly.polyfit(fitlistx_logchop, fitlisty_logchop, 1) # Define a polynomial to fit the events in log space\n",
    "                            ffit = poly.polyval(fitlistx_logchop, coefs) # Creating the fit in log space\n",
    "                            xfit = np.exp(fitlistx_logchop) # Convert back from log\n",
    "                            ffitexp = np.exp(ffit)\n",
    "                            function1 = []\n",
    "                            function2 = []\n",
    "                            function3 = []\n",
    "                            ffitexp_total = np.exp(poly.polyval(np.log(totalfitx), coefs))\n",
    "                            ffitexp_totalmod = []\n",
    "                            # this loop is to fix the issue of the start of the exponential fits going way above the peak in the data\n",
    "                            for k in range(len(ffitexp_total)):\n",
    "                                if ffitexp_total[k] > np.max(fitlistsy[i]):\n",
    "                                    ffitexp_totalmod.append(np.max(fitlistsy[i]))\n",
    "                                else:\n",
    "                                    ffitexp_totalmod.append(ffitexp_total[k])\n",
    "\n",
    "                            ffitexp_total[:phase_index+3] = np.max(fitlistsy[i])\n",
    "                            \n",
    "                            # Re-defining combined functions based off of new exponential\n",
    "                            for l in range(len(totalfitx)): \n",
    "                                function1.append((1 - sigma1[l]) * ffit_re_total[l]+sigma1[l] * ffit_p_total[l])\n",
    "                                function2.append((1 - sigma2[l]) * ffit_p_total[l]+sigma2[l] * ffitexp_totalmod[l])\n",
    "                                function3.append((1 - sigma1[l]) * ffit_re_total[l] +sigma1[l] * function2[l])\n",
    "                            print(\"failed fit found, correcting\")\n",
    "                            \n",
    "                            maxval = np.max(function3)\n",
    "                            maxvalindex = function3.index(maxval)\n",
    "                            onesigma = maxval*(1/np.exp(1))\n",
    "                            sigmaminx = totalfitx[ffitexp_totalmod.index(find_nearest(ffitexp_totalmod,onesigma))]\n",
    "                            sigmaminy = function3[ffitexp_totalmod.index(find_nearest(ffitexp_totalmod,onesigma))]\n",
    "                            thistau = 1*(sigmaminx - totalfitx[maxvalindex])\n",
    "                            timeconsts[j].append(thistau)\n",
    "                            \n",
    "                        else:\n",
    "                            timeconsts[j].append(thistau)\n",
    "                        \n",
    "                        # If there's still an issue then this skips over the pulse and appends the error array\n",
    "                        if thistau < 0.0:\n",
    "                            print(\"could not correct\")\n",
    "                            errors.append(1)\n",
    "                        \n",
    "                        \n",
    "                        # Show each fit\n",
    "                        if showfits:\n",
    "                            pl.clf()\n",
    "                            pl.figure()\n",
    "                            pl.title('dataset: ' + str(datalist[n][-7:-4]) + ', channel: ' + str(SQs[j]))\n",
    "                            pl.plot(totalfitx, totalfity)\n",
    "                            pl.plot(totalfitx, function3, 'r')\n",
    "                            pl.pause(0.05)\n",
    "                            pl.show(block=False)\n",
    "                        \n",
    "                        \n",
    "                        # Save each fit\n",
    "                        if savefits:\n",
    "                            newdir = os.path.join(writepath+'fitplots/')\n",
    "                            if not os.path.exists(newdir):\n",
    "                                os.mkdir(newdir)\n",
    "                            pl.clf()\n",
    "                            pl.figure()\n",
    "                            pl.title('dataset: ' + str(datalist[n][-7:-4]) + ', channel: ' + str(SQs[j]))\n",
    "                            pl.plot(totalfitx, totalfity)\n",
    "                            pl.plot(totalfitx, function3, 'r')\n",
    "                            pl.savefig(newdir + wpsuffix + '_dataset-' + str(datalist[n][-7:-4]) + '_channel-' + str(SQs[j])+  '.png')\n",
    "                 \n",
    "        \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        # Coincidence finding! \n",
    "        # This still needs to be worked on - loops in current method add significant compute time to analysis (for triple coincidence)\n",
    "        \n",
    "        if coincidence:\n",
    "            meantcs = []\n",
    "            meantimeconst = []\n",
    "            for j in range(len(SQs)):\n",
    "                meantcs.append(np.mean(timeconsts[j]))\n",
    "            meantimeconst = np.mean(meantcs)\n",
    "\n",
    "            if len(SQs)==3:\n",
    "                coinc01 = []\n",
    "                coinc01_time = []\n",
    "                coinc02 = []\n",
    "                coinc02_time = []\n",
    "                coinc12 = []\n",
    "                coinc12_time = []\n",
    "                coinc012 = []\n",
    "  \n",
    "\n",
    "        # If two peaks in different channels are within three time constants\n",
    "                for i in range(len(peakvals[0])):\n",
    "                    for j in range(len(peakvals[1])):\n",
    "                        if np.abs(peakvals[0][i] - peakvals[1][j]) < meantimeconst*3:\n",
    "                            coinc01.append(datalist[n][-7:-4])\n",
    "                            coinc01_time.append(peakvals[0][i] - peakvals[1][j])\n",
    "\n",
    "                for i in range(len(peakvals[0])):\n",
    "                    for j in range(len(peakvals[2])):\n",
    "                        if np.abs(peakvals[0][i] - peakvals[2][j]) < meantimeconst*3:\n",
    "                            coinc02.append(datalist[n][-7:-4])\n",
    "                            coinc02_time.append(peakvals[0][i] - peakvals[2][j])\n",
    "\n",
    "                for i in range(len(peakvals[1])):\n",
    "                    for j in range(len(peakvals[2])):\n",
    "                        if np.abs(peakvals[1][i] - peakvals[2][j]) < meantimeconst*3:\n",
    "                            coinc12.append(datalist[n][-7:-4])\n",
    "                            coinc12_time.append(peakvals[1][i] - peakvals[2][j])\n",
    "\n",
    "                allcoinc = [coinc01, coinc01_time, coinc02, coinc02_time, coinc12, coinc12_time, coinc012]\n",
    "                print(allcoinc)\n",
    "        \n",
    "        \n",
    "        \n",
    "     # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "    \n",
    "        # Defines an array for the hit rates in each channel\n",
    "        \n",
    "        ratesall = [sum(rates[0])/(len(datalist)*scantime), sum(rates[1])/(len(datalist)*scantime), sum(rates[2])/(len(datalist)*scantime)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        \n",
    "        jfe_datasets.append(jfe_channels) # Appending the JFE data sets in the fit loop\n",
    "        \n",
    "    # End of the fit loop\n",
    "    \n",
    "    fe_array = np.array(jfe_datasets)\n",
    "    totaljfe = np.sum(jfe_array)\n",
    "    TRE = np.sum(jfe_datasets)\n",
    "    numerrors = sum(errors)\n",
    "\n",
    "    # Write all pertinent information to pickle files\n",
    "    \n",
    "    with open(writepath + 'rates_' + wpsuffix, 'wb') as fp1:\n",
    "        pickle.dump(ratesall, fp1)\n",
    "    \n",
    "    with open(writepath + 'energies_' + wpsuffix, 'wb') as fp2:\n",
    "        pickle.dump(energies, fp2)\n",
    "        \n",
    "    with open(writepath + 'energies-data_' + wpsuffix, 'wb') as fp3:\n",
    "        pickle.dump(energies_data, fp3)\n",
    "        \n",
    "    with open(writepath + 'timeconsts_' + wpsuffix, 'wb') as fp4:\n",
    "        pickle.dump(timeconsts, fp4)\n",
    "        \n",
    "    with open(writepath + 'peakvals_' + wpsuffix, 'wb') as fp5:\n",
    "        pickle.dump(peakvals, fp5)\n",
    "        \n",
    "    if coincidence:\n",
    "        with open(writepath + 'coincidence_' + wpsuffix, 'wb') as fp6:\n",
    "            pickle.dump(allcoinc, fp6)\n",
    "        print(\"allcoinc:\", allcoinc)\n",
    "    \n",
    "    with open(writepath + 'jfedatasets_' + wpsuffix, 'wb') as fp7:\n",
    "        pickle.dump(jfe_datasets, fp7)\n",
    "\n",
    "    with open(writepath + 'errors_' + wpsuffix, 'wb') as fp8:\n",
    "        pickle.dump(numerrors, fp8)\n",
    "    print(\"numerrors:\", numerrors)\n",
    "        \n",
    "    \n",
    "    # Saving figures\n",
    "    \n",
    "    if savefigs:\n",
    "        \n",
    "        transp = 0.75\n",
    "        # energies from fits\n",
    "        pl.clf() \n",
    "        n1, bins1, patchs1 = pl.hist(energies[0], bins='auto', alpha = transp, log=True,  label = 'bolo 1')\n",
    "        n2, bins2, patchs2 = pl.hist(energies[1], bins='auto', alpha = transp, log=True, label = 'bolo 2')\n",
    "        n3, bins3, patchs3 = pl.hist(energies[2], bins='auto', alpha = transp, log=True, label = 'bolo 3')\n",
    "        #pl.xscale('log')\n",
    "        #pl.xlim(xmin = 0., xmax =200.)\n",
    "        pl.title(wpsuffix)\n",
    "        pl.xlabel(\"Pulse energy (arb. units)\")\n",
    "        pl.legend(loc='upper right')\n",
    "        #pl.show()\n",
    "        pl.savefig(writepath + 'energiesfromfits_' + wpsuffix + '.png')\n",
    "        \n",
    "        # energies from data\n",
    "        pl.clf() \n",
    "        n1, bins1, patchs1 = pl.hist(energies_data[0], bins='auto', alpha = transp, log=True,  label = 'bolo 1')\n",
    "        n2, bins2, patchs2 = pl.hist(energies_data[1], bins='auto', alpha = transp, log=True, label = 'bolo 2')\n",
    "        n3, bins3, patchs3 = pl.hist(energies_data[2], bins='auto', alpha = transp, log=True, label = 'bolo 3')\n",
    "        #pl.xscale('log')\n",
    "        #pl.xlim(xmin = 0., xmax =200.)\n",
    "        pl.title(wpsuffix)\n",
    "        pl.xlabel(\"Pulse energy (arb. units)\")\n",
    "        pl.legend(loc='upper right')\n",
    "        #pl.show()\n",
    "        pl.savefig(writepath + 'energiesfromdata_' + wpsuffix + '.png')\n",
    "        \n",
    "        # timeconstants\n",
    "        pl.clf() \n",
    "        n1, bins1, patchs1 = pl.hist(timeconsts[0], bins='auto', alpha = transp, log=True,  label = 'bolo 1')\n",
    "        n2, bins2, patchs2 = pl.hist(timeconsts[1], bins='auto', alpha = transp, log=True, label = 'bolo 2')\n",
    "        n3, bins3, patchs3 = pl.hist(timeconsts[2], bins='auto', alpha = transp, log=True, label = 'bolo 3')\n",
    "        #pl.xscale('log')\n",
    "        #pl.xlim(xmin = 0., xmax =200.)\n",
    "        pl.title(wpsuffix)\n",
    "        pl.xlabel(\"Pulse energy (arb. units)\")\n",
    "        pl.legend(loc='upper right')\n",
    "        #pl.show()\n",
    "        pl.savefig(writepath + 'timeconstants_' + wpsuffix + '.png')\n",
    "        \n",
    "        \n",
    "        # writing values of interest to a text file\n",
    "        \n",
    "        txtfile = open(writepath + 'vals3_' + wpsuffix + '.txt', 'w')\n",
    "        \n",
    "        txtfile.write(\"Total number of events in \" + str((len(datalist)*scantime)/3600.) + \" hours of data: \" + str(len(energies[0]+energies[1]+energies[2])) + \" \\n \\n \\n\")\n",
    "        \n",
    "        txtfile.write(\"Jumped flux events: \\n\")\n",
    "        txtfile.write(\"Total number of JFEs in bolo 1: \"+ str(np.sum(jfe_array, axis=0)[0]) + \" \\n\")\n",
    "        txtfile.write(\"Total number of JFEs in bolo 2: \"+ str(np.sum(jfe_array, axis=0)[1]) + \" \\n\")\n",
    "        txtfile.write(\"Total number of JFEs in bolo 3: \"+ str(np.sum(jfe_array, axis=0)[2]) + \" \\n\")\n",
    "        txtfile.write(\"Total number of JFEs in all detectors: \"+ str(np.sum(jfe_array)) + \" \\n\")\n",
    "        if len(energies[0]+energies[1]+energies[2]) != 0:\n",
    "            txtfile.write(\"% of total events that were jumped flux: \"+ str(100.*totaljfe/len(energies[0]+energies[1]+energies[2])) + \"% \\n \\n \\n\")\n",
    "        else:\n",
    "            txtfile.write(\"not enough data \\n \\n \\n\")\n",
    "            \n",
    "        txtfile.write(\"Rates \\n\")\n",
    "        txtfile.write(\"bolo 1 rate: \" + str(ratesall[0]) + \" Hz \\n\")\n",
    "        txtfile.write(\"bolo 2 rate: \" + str(ratesall[1]) + \" Hz \\n\")\n",
    "        txtfile.write(\"bolo 3 rate: \" + str(ratesall[2]) + \" Hz \\n\")\n",
    "        if len(ratesall) != 0:\n",
    "            txtfile.write(\"Average rate of all three channels: \" + str(sum(ratesall)/len(ratesall)) + \" Hz \\n \\n \\n\")\n",
    "        else:\n",
    "            txtfile.write(\"not enough data \\n \\n \\n\")\n",
    "        \n",
    "        \n",
    "        txtfile.write(\"Time constants \\n\")\n",
    "        txtfile.write(\"Mean time constant in bolo 1: \" + str(timeconsts[0]) + \" s \\n\")\n",
    "        txtfile.write(\"Mean time constant in bolo 2: \" + str(timeconsts[1]) + \" s \\n\")\n",
    "        txtfile.write(\"Mean time constant in bolo 3: \" + str(timeconsts[2]) + \" s \\n\")\n",
    "        \n",
    "        if len(timeconsts[0]+timeconsts[1]+timeconsts[2]) != 0:\n",
    "            txtfile.write(\"Average time constant in all three channels: \" + str(sum(timeconsts[0]+timeconsts[1]+timeconsts[2])/(len(timeconsts[0]+timeconsts[1]+timeconsts[2]))) + \" seconds \\n \\n \\n\")\n",
    "        else:\n",
    "            txtfile.write(\"not enough data \\n \\n \\n\")\n",
    "            \n",
    "        if coincidence:\n",
    "            txtfile.write(\"Number of double coincidence events:\" + str(len(allcoinc[0])+len(allcoinc[2])+len(allcoinc[4]))+ \"\\n\")\n",
    "            txtfile.write(\"Number of triple coincidence events: N/A \\n \\n \\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        txtfile.write(\"Number of fit errors: \\n\")\n",
    "        txtfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41e5b821-ba05-4cc1-8631-f161d068d985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 20210516 - control bolos, no source\n",
      "running data set 002\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 003\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 004\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 005\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 006\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 007\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 008\n",
      "channel 1\n",
      "channel 2\n",
      "channel 6\n",
      "running data set 009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6ac638f3671a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running 20210516 - control bolos, no source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_05_16-control_nosource/CR_Scan_002/_\"], \\\n\u001b[0m\u001b[1;32m     11\u001b[0m            [2,259], [1,2,6], [100.,100.,100.], [3.457,3.187,3.165], 300., [False, False, True], [\"20210516_controlbolos_nosource_2-256\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f3e2c9c402a8>\u001b[0m in \u001b[0;36mCR_analyze\u001b[0;34m(data, sets, SQs, gain, bias, scantime, inv, pklsuffix, savefits, savefigs, showfits, fluxfix, noisefilt, coincidence, verbose, detrend, pwrcnv, fit)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_raw_ts_3ch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatalist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mdList0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-05ad9b8b1db6>\u001b[0m in \u001b[0;36mextract_raw_ts_3ch\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_raw_ts_3ch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mcolNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'y0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mextract_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_raw_ts_1ch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-05ad9b8b1db6>\u001b[0m in \u001b[0;36mextract_raw_data\u001b[0;34m(fn, colNames)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mllist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlineSplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(\"Running 2021_03_19 - bolo 1: heatsunk control, bolo 2: floating control, bolo 3: floating bling, no source\")\n",
    "\n",
    "#CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_03_19/CR_Scan_023/20210319_SQ1-contr1_SQ2-contr2_SQ6-contr3_biasthreeturnsintotrans_5-FTR-5min_\"], \\\n",
    "           #[44,999], [1,2,6], [100.,100.,100.], [0.34,0.365,0.367], 300., [True, True, True], [\"20210319_b1-hscontr_b2-fltcontr_b3-fltblng_nosource_44-999\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "#print(\"Running 2021_03_23 - bolo 1: heatsunk control, bolo 2: floating control, bolo 3: floating bling, source\")\n",
    "#CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_03_23/CR_Scan_002/20210319_SQ1-contr1_SQ2-contr2_SQ6-contr3_biasthreeturnsintotrans_5-FTR-5min_Co60_\"], \\\n",
    "          #[1,847], [1,2,6], [100.,100.,100.], [0.34,0.365,0.367], 300., [True, True, True],[\"20210323_b1-hscontr_b2-fltcontr_b3-fltblng_Co60_1-847\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "print(\"Running 20210516 - control bolos, no source\")\n",
    "CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_05_16-control_nosource/CR_Scan_002/_\"], \\\n",
    "           [2,259], [1,2,6], [100.,100.,100.], [3.457,3.187,3.165], 300., [False, False, True], [\"20210516_controlbolos_nosource_2-256\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "print(\"Running 20210517 - control bolos, source\")\n",
    "CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_05_17-control_source/CR_Scan_002/source_\"], \\\n",
    "           [1,824], [1,2,6], [100.,100.,100.], [3.517,3.164,3.150], 300., [False, False, True], [\"20210517_controlbolos_Co-60_1-824\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "print(\"Running 20210528 - mitigated bolos, source\")\n",
    "CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_05_28-mitig_source/CR_Scan_003/_\"], \\\n",
    "           [1,727], [1,2,6], [100.,100.,100.], [2.809,2.911,1.48], 300., [False, False, True], [\"20210528_controlbolos_Co-60_1-727\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "print(\"Running 20210604 - mitigated bolos, no source\")\n",
    "CR_analyze([\"/Users/shawn/Google Drive/My Drive/Python/Cosmic Rays/2021_06_04-mitig_nosource/CR_Scan_002/BT10-02-CR-with-mitigation-no-source_\"], \\\n",
    "           [1,999], [1,2,6], [100.,100.,100.], [2.809,2.911,1.48], 300., [False, False, True], [\"20210604_controlbolos_nosource_1-999\"], showfits=False, noisefilt=False, verbose = False, fit = True, pwrcnv = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d126dfb-1299-49f2-82da-821c46f76905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
